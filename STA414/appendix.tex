\section{Appendix}
\subsection{Example 1}
\label{example-1}
The Bernoulli Na√Øve Bayes model parameterized by $\theta$ and $\pi$ defines the following joint probability of $x$ and $c$,
$$p(x,c|\theta,\pi) = p(c|\pi)p(x|c,\theta) = p(c|\pi)\prod_{j=1}^{D}p(x_j|c,\theta),$$
where $x_j | c,\theta \sim \operatorname{Bernoulli}(\theta_{jc})$, i.e. $p(x_j | c,\theta) = \theta_{jc}^{x_j}(1-\theta_{jc})^{1-x_j}$, and $c|\pi$ follows a simple categorical distribution, i.e. $p(c|\pi) = \pi_c$.\\
\textbf{Solution}:\\
For $\mathbf{x^1}$, its likelihood function is:
\begin{align}
    L(\theta, \pi; x^1, c) &= \prod_{c = 1}^{10} \left[ p(x^1, c \mid \theta, \pi) \right]^{1\{ c^1 = c \}} \\
    &= \prod_{c = 1}^{10} \left[ p(c \mid \pi) \prod_{j = 1}^{784} p(x_j^1 \mid c, \theta) \right]^{1\{ c^1 = c \}} \\
    &= \prod_{c = 1}^{10} \left[ \pi_c \prod_{j = 1}^{784} \theta_{jc}^{x_j^1} (1-\theta_{jc})^{1-{x_j^1}} \right]^{1\{ c^1 = c \}}
\end{align}
Therefore, the joint likelihood function for $\mathbf{x^1}, \cdots \mathbf{x^n}$ is:
\begin{align}
    L(\theta, \pi) &= \prod_{i = 1}^{n} \prod_{c = 1}^{10} \left[ \pi_c \prod_{j = 1}^{784} \theta_{jc}^{x_j^i} (1-\theta_{jc})^{1-{x_j^i}} \right]^{1\{ c^i = c \}} \\
    \Rightarrow l(\theta, \pi) &= \log \left( \prod_{i = 1}^{n} \prod_{c = 1}^{10} \left[ \pi_c \prod_{j = 1}^{784} \theta_{jc}^{x_j^i} (1-\theta_{jc})^{1-{x_j^i}} \right]^{1\{ c^i = c \}} \right) \\
    &= \sum_{i = 1}^{n} \sum_{c = 1}^{10} 1\{ c^i = c \} \left\{ \log(\pi_c) + \sum_{j = 1}^{784} \left[ x_j^i \log(\theta_{jc}) + (1-x_j^i) \log(1-\theta_{jc}) \right] \right\} \\
    &= \sum_{i = 1}^{n} \sum_{c = 1}^{9} 1\{ c^i = c \} \left\{ \log(\pi_c) + \sum_{j = 1}^{784} \left[ x_j^i \log(\theta_{jc}) + (1-x_j^i) \log(1-\theta_{jc}) \right] \right\} \\
    &+ \sum_{i = 1}^{n} 1\{ c^i = 10 \} \left\{ \log(1 - \sum_{c = 1}^{9}\pi_{c}) + \sum_{j = 1}^{784} \left[ x_j^i \log(\theta_{j,10}) + (1-x_j^i) \log(1-\theta_{j,10}) \right] \right\}
\end{align}
If we pick any $c \in [C]$ and $j \in [D]$:
\begin{align}
    \frac{\partial l(\theta, \pi)}{\partial \theta_{jc}} = \sum_{i = 1}^{n} 1\{ c^i = c \} \left( \frac{x_j^i}{\theta_{jc}} - \frac{1-x_j^i}{1-\theta_{jc}} \right)
\end{align}
Letting it equal to zero, we have:
\begin{align}
    \hat{\theta_{jc}} = \frac{\sum_{i = 1}^{n} 1\{ c^i = c \} x_j^i}{\sum_{i = 1}^{n} 1\{ c^i = c \}}
\end{align}
For $\pi_c$:
\begin{align}
    \frac{\partial l(\theta, \pi)}{\partial \pi_c} = \sum_{i = 1}^{n} 1\{ c^i = c \} \frac{1}{\pi_c} - \sum_{i = 1}^{n} 1\{ c^i = 10 \} \frac{1}{1 - \sum_{c = 1}^{9} \pi_c}
\end{align}
Letting $n_c = \sum_{i = 1}^{n} 1\{ c^i = c \}$, when it equals to zero, we have:
\begin{align}
    n_c (1 - \sum_{c = 1}^{9} \hat{\pi_c}) = \hat{\pi_c} n_{10}, \quad \text{where } 1 \leq a \leq 9
\end{align}
Summation on both sides over $1 \leq c \leq 9$, we have:
\begin{align}
    \sum_{c = 1}^{9} n_c (1 - \sum_{c = 1}^{9} \hat{\pi_c}) = \sum_{c = 1}^{9} \hat{\pi_c} n_{10}
\end{align}
\begin{align}
    \Rightarrow (n - n_{10})(1 - \sum_{c = 1}^{9} \hat{\pi_c}) = n_{10} \sum_{c = 1}^{9} \hat{\pi_c}
\end{align}
\begin{align}
    \sum_{c = 1}^{9} \hat{\pi_c} = \frac{n - n_{10}}{n}
\end{align}
Substituting back, we will have:
\begin{align}
    \hat{\pi_c} = \frac{n_c}{n}, \quad 1 \leq c \leq 9
\end{align}

\subsection{Example 2}
\label{example-2}
We can write this distribution as an exponential family
\begin{align}
p(x \mid \theta) &= \theta^x (1 - \theta)^{1 - x}\\
    &= \exp \left\{ x \log(\theta) + (1 - x) \log(1 - \theta) \right\}\\
    &= \exp \left\{ x \log \left( \frac{\theta}{1 - \theta} \right) + \log(1 - \theta) \right\}
\end{align}

Here,

\begin{align*}
    T(x) &= x \\
    \eta &= \log \left( \frac{\theta}{1 - \theta} \right) \\
    A(\eta) &= \log(1 + e^{\eta}) \\
    h(x) &= 1
\end{align*}
Notice that \( A'(\eta) = \frac{e^\eta}{1 + e^\eta} = \theta \) is the mean of \( T(X) = X \) and \( A''(\eta) = \frac{e^\eta}{(1 + e^\eta)^2} = \theta (1 - \theta) \) is the variance of \( X \).

\subsection{Derivations 1}
\label{derivations-1}
We add and subtract $\mathbb{E}[t \mid x]$ and write
$$
\begin{aligned}
\mathbb{E}[L]= & \iint(y(x)-t)^2 p(x, t) d x d t \\
= & \iint(y(x)-\mathbb{E}[t \mid x]+\mathbb{E}[t \mid x]-t)^2 p(x, t) d x d t \\
= & \iint(y(x)-\mathbb{E}[t \mid x])^2 p(x, t) d x d t+\iint(\mathbb{E}[t \mid x]-t)^2 p(x, t) d x d t \\
& +2 \iint(y(x)-\mathbb{E}[t \mid x])(\mathbb{E}[t \mid x]-t) p(x, t) d x d t
\end{aligned}
$$

The last term is zero since
$$
\begin{aligned}
& \iint(y(x)-\mathbb{E}[t \mid x])(\mathbb{E}[t \mid x]-t) p(x, t) d x d t \\
& =\iint(y(x)-\mathbb{E}[t \mid x])(\mathbb{E}[t \mid x]-t) p(t \mid x) p(x) d x d t \\
& =\int(y(x)-\mathbb{E}[t \mid x])\{\underbrace{\int(\mathbb{E}[t \mid x]-t) p(t \mid x) d t}_{=0}\} p(x) d x=0
\end{aligned}
$$