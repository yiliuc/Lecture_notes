\section{Markov Chain Monte Carlo}
\subsection{Introduction to Markov Chains}
In most cases, we assume the samples are independent. However, it is a poor assumption as this may rarely happen in reality. Therefore, we assume the data are dependent and hence we have \textbf{sequential data}. More specifically, we can use \textbf{markov chain} to model the sequential data.
\begin{figure}[H]
    \centering
    \includegraphics[width = .6\linewidth]{codes/figures/section6/figure_6_1.png}
    \caption{First order Markov Chain}
    \label{fig:f_mc}
\end{figure}
\hyperref[fig:f_mc]{Figure 6.1} shows an example of First order Markov chain.
\begin{align*} 
    p\left(x_t | x_{1: t-1}\right)&=p\left(x_t | x_{t-1}\right)\\
    &=\prod_{t=1}^T p\left(x_t | x_{t-1}\right)
\end{align*}
From the graph, we can see that the first order means each node is conditioned on the previous node. We can therefore generalize to higher order Markov Chains.
\begin{itemize}    \item For second-order Markov Chains:
    $$p\left(x_t | x_{1: t-1}\right)=p\left(x_t | x_{t-1},x_{t-2}\right)$$
    \item For m-order Markov Chains:
    $$p\left(x_{1: T}\right)=p\left(x_t | x_{t-1:t-m}\right)$$
\end{itemize}
We have two further definations:\\
\textbf{Stationary(homongeneous) Markov Chain}: The distribution generating the data does not change over time:
$$p(x_{t+1}=y|x_t=x)=p(x_{t+2}=y|x_{t+1}=x)$$ 
\textbf{Non-stationary Markov Chain}: $p(x_{t+1}=y|x_t=x)$ depends on time $t$.

\subsubsection*{Transition Matrix}
If we assume $x_t$ is discrete: $x_t\in\{1,2,\cdots,K\}$. Then the conditional distribution $p(x_t|x_{t-1})$ can be written as a $K\times K$ matrix, which is called the \textbf{transition (stochastic) matrix $A$}. Each element on $A$ represents the probability of transition from $t-1$ to $t$, i.e.
$$A_{ij}=p(x_t=j|x_{t-1}=i),\:A\in K\times K$$
We also have:
\begin{align*} 
    p\left(x_t=j\right) & =\sum_i p\left(x_t=j | x_{t-1}=i\right) p\left(x_{t-1}=i\right) \\ 
    & =\sum_i A_{i j} p\left(x_{t-1}=i\right)
\end{align*}
The sum for each row in $A$ equals to \textbf{1} as it represent all the possibilities of changing from $t-1$ to $t$ ginve $i$ at $t-1$.
$$\sum_i A_{i j}=1$$
\textit{An example of transition matrix can be found \hyperref[fig:transition-example]{here}.}\\
The $n$-step transition matrix $A(n)$is the probability of transferring from two states in $n$ steps, where $n$ can be considered as \textit{"n"} times. In equation:
$$
A_{i j}(n)=p\left(x_{t+n}=j | x_t=i\right)
$$
Since the one-step transition matrix considers $t-1$ and $t$ (or equivalently $t$ and $t+1$)
$$A(1)=A$$
\textbf{Chapman-Kolmogorov equations}:
\begin{align*}
    A_{i j}(m+n)&=\sum_{k=1}^K A_{i k}(m) A_{k j}(n)\\
    \Leftrightarrow A(m+n)&=A(m) A(n)
\end{align*}

It says we can find an intermediate state $k$ between $i$ and $j$, which takes $m$ steps from $i$ to $k$, and $n$ steps from $k$ to $j$. It is similar to lattice paths on combinatorics.\\
$\Rightarrow A(n)=A(1+n-1)=A \cdot A(n-1)=A \cdot A \cdot A(n-2)=\cdots=A^n$.

\subsubsection*{Markov Language Models}
Language models can be represented as Markov Chains. That said, each word is a state, and the probability of transition from one word to another can be also represented as transition matrix.

\begin{align*} 
    p\left(x_{1: T} | \theta\right) & =\pi\left(x_1\right) A\left(x_1, x_2\right) \cdots A\left(x_{T-1}, x_T\right) \\
    & =\underbrace{\prod_{j=1}^K \pi_j^{1\left[x_1=j\right]}}_{\text{prob. starting with }x_1} \prod_{t=2}^T \prod_{j=1}^K \prod_{k=1}^K A_{j k}^{1\left[x_t=k, x_{t-1}=j\right]}
\end{align*}

\subsection{Stationary distribution of a (homongeneous) Markov Chain}
The stationary\footnote{the "stationary" in "stationary distribution" is not as same as in "stationary Markov chain." Therefore, we usually called it as homongeneous Markov chains.} distribution is the \textbf{long-term} distribution over states. It is also called the \textbf{steady state}. In notation, we have $\pi_t(j)=p(x_t=j)$, to be the probability of being in state $j$ on time $t$.\\
\begin{itemize}
    \item We have the initial distribution which is given by $\pi_0\in\mathbb{R}^{K}$
    \item We then represent $\pi_1$ as:
    $$\pi_1(j)=\sum_{i=1}^K p\left(x_1=j | x_0=i\right) \pi_0(i)=\sum_{i=1}^K A_{i j} \pi_0(i)=\sum_{i=1}^K\left(A^{\top}\right)_{j i} \pi_0(i)$$
    $$\Rightarrow \pi_1=A^T\pi_0$$
    \item We can therefore generalize to $\pi_t$.
    $$\pi_t=A^{\top} \pi_{t-1}=A^{\top} A^{\top} \pi_{t-2}=\cdots=\left(A^{\top}\right)^t \pi_0$$
    \item If we do this for many steps; eventually, the distribution of $\pi_t$ may converge:
    $$\pi=A^T\pi$$
    which is the stationary distribution of the Markov chain\footnote{An example of how to compute the stationary distribution can be found \hyperref[sec:example-stationary]{here}.}
\end{itemize}

\textbf{Recall: Eigenvalues and Eigenvectors}\\
Let $T$ be a transformation, $\mathbf{v}$ be a vector and $\lambda$ be a constant. If we have:
$$T\mathbf{v}=\lambda\mathbf{v}$$
Then $\mathbf{v}$ is the eigenvector of $T$ and $\lambda$ is the eigenvalue.\\

Therefore, it follows that the statioanary distribution $\pi$ is an eigenvector of $A^T$ with eigenvalue 1. In other words, $A$ and $A^T$ have the same eigenvalues and $A\mathbf{1}=\mathbf{1}$ as the row sum of $A$ is 1. Thus, due to the equality of characteristics polynomials, 1 is also the eigenvalue of $A^T$. More importantly, \colorbox{yellow}{\textbf{stationary distribution is NOT unique}}.\\
\subsubsection*{Detailed balance equation}
Markov Chain is called:
\begin{itemize}
    \item \textbf{irreducible} if we can get from any state to any other state.
    \item \textbf{regular} if $A^n$ has positive entries for some $n$.
    \item \textbf{time reversible} if there exists a distribution $\pi$ such that
    $$
    \underbrace{\pi_i A_{i j}=\pi_j A_{j i}}_{\textbf{detailed balance equation}} \quad \text { for all } i, j .
    $$
    In other words, detailed balance means the probability of transition from one state to another and reverse back is \textbf{equally possible}.
\end{itemize}
\begin{theorem}
    If a markov chain with transition matrix $A$ satisfies the detailed balance with respect to distribution $\pi$, then $\pi$ is a stationary distribution.
\end{theorem}
Recall in rejection sampling, we accept the samples independently. In contrast, we assume the samples are \textbf{dependent}, where each sample is a state in the Markov chain and $x^{(t)}$ has a probability distribution depending on $x^{t-1}$. In other words, we will \textbf{learn} from the previous samples. Hence the stationary distribution will be $p(x)$.\\

\subsection{Metropolis-Hasting Algorithms}

Compared to rejection sampling, metropolis-hasting prose a simple density $q$ which depends on the current state $x^{(t)}$. If $q$ is Gaussian, then its center will be $x^{(t)}$. As before, assume we can evaluate $\tilde{p}(x)$ for any $x$. The algorithm follows as:
\begin{itemize}
    \item We firstly have a tentative new state $x^{\prime}$ is generated from the proposal density $q\left(x^{\prime} | x^{(t)}\right)$. We accept the new state with probability
    \item We then compute:
    $$
    A\left(x^{\prime}|x^{(t)}\right)=\min \Biggl\{1, \underbrace{\frac{\tilde{p}\left(x^{\prime}\right) q\left(x^{(t)}|x^{\prime}\right)}{\tilde{p}\left(x^{(t)}\right) q\left(x^{\prime} | x^{(t)}\right)}}_{\mathcal{B}}\Biggl\}
    $$
    \item for $\mathcal{B}$
    \begin{itemize}
        \item If $\mathcal{B}>1$, then accept the new state $\prime{x}$.
        \item If $\mathcal{B}<1$, we sample $u\sim N(0,1)$. Accept if $u\leq\mathcal{B}$; otherwise reject.
    \end{itemize}
\end{itemize}
\textbf{Metropolis algorithm}: Simpler version when $q\left(x^{\prime} | x\right)=q\left(x \mid x^{\prime}\right)$ for all $x, x^{\prime}$.
\begin{theorem}
    This procedure defines a Markov chain with stationary distribution $\pi(x)$ equal to the target distribution $p(x)$. $\pi(x)=p(x)$. \textit{proof \hyperref[sec:proof-metro]{here}}
\end{theorem}

\subsection{Gibbs Samplign Procedure}
Gibbs sampling is used when we have two or more dimensions to sample. For example, for bivariate gaussian distribution, $p(x,y)$ is difficult to compute but the conditional distribution $p(y|x)$ and $p(x|y)$ are easier. It works as follows:
\begin{itemize}
    \item Suppose the vector $x$ is d-dimensional
    $$
    x=\left(x_1, \ldots, x_d\right) .
    $$
    \item Start with any $x^{(0)}=\left(x_1^{(0)}, \ldots, x_d^{(0)}\right)$. For $j=1, \ldots, d$ :
    \begin{itemize}
        \item Sample $x_j^{(t)}$ from the conditional distribution given other components:
        $$
        x_j^{(t)} \sim p\left(x_j \mid x_{-j}^{(t-1)}\right)
        $$
        Where $x_{-j}^{(t-1)}$ represents all the components of $x$ except for $x_j$ at their current values:
        $$
        x_{-j}^{(t-1)}=\left(x_1^{(t)}, x_2^{(t)}, \ldots, x_{j-1}^{(t)}, x_{j+1}^{(t-1)}, \ldots, x_d^{(t-1)}\right)
        $$
    \end{itemize}
   
    \item No accept/reject, only accept.
\end{itemize}
In bivariate gaussian cases, we just sample $y$ and $x$ iteratively.
\subsubsection*{Recap: Conditional Distributio on Bivariate Gaussian}
Suppose we have two random variables $X_1$ and $X_2$, which follow Gaussian Distribution.
$$
X=\left[\begin{array}{l}
X_1 \\
X_2
\end{array}\right] \sim N_2(\mu, \Sigma), \quad \mu=\left[\begin{array}{l}
\mu_1 \\
\mu_2
\end{array}\right], \quad \Sigma=\left[\begin{array}{ll}
1 & \rho \\
\rho & 1
\end{array}\right] .
$$

Then:
$$
\begin{aligned}
& X_1 \mid X_2=x_2 \sim N\left(\mu_1+\rho\left(x_2-\mu_2\right), 1-\rho^2\right) \\
& X_2 \mid X_1=x_1 \sim N\left(\mu_2+\rho\left(x_1-\mu_1\right), 1-\rho^2\right)
\end{aligned}
$$

Given $X^{(0)}=(0,0)$ we proceed iteratively for $t \geq 1$ :
$$
\begin{gathered}
X_1^{(t)} \sim N\left(\mu_1+\rho\left(x_2^{(t-1)}-\mu_2\right), 1-\rho^2\right) \\
X_2^{(t)} \sim N\left(\mu_2+\rho\left(x_1^{(t)}-\mu_1\right), 1-\rho^2\right)
\end{gathered}
$$

\subsection{Hamiltonian Monte Carlo}
Hamiltonian Monte Carlo is a specialized Metropolis-Hasting algorithm. We will use physical analogy to make samples.
\begin{itemize}
    \item We assume the \textbf{potential energy} for each data point is $E(x)$.
    \item The distribution $p(x)$:
    $$p(x) \propto e^{-E(x)}, \quad\text{with}\quad E(x)=-\log (\tilde{p}(x))$$
    \item Introduce \textbf{momentum} $v$ carrying the kinetic energy
    $$
    K(v)=\frac{1}{2}\|v\|^2=\frac{1}{2} v^{\top} v .
    $$
    \item The \textbf{total energy/hamiltonian} is the sum of potential energy and kinetic energy
    $$
    H(x, v)=E(x)+K(v) .
    $$
    \item The joint distribution will be:
    $$p(x, v) \propto e^{-E(x)} e^{-K(v)}=e^{-E(x)-K(v)}=e^{-H(x, v)}$$
    \item Same as physic, we assume the energy is preserved. That said:
    \begin{itemize}
        \item Frictionless ball rolling $(x, v) \rightarrow\left(x^{\prime}, v^{\prime}\right)$
        \item $H(x, v)=H\left(x^{\prime}, v^{\prime}\right)$.
    \end{itemize}
    \item Ideal Hamiltonian dynamics are reversible: reverse $v$ and the ball will return to its start point! $\quad(x, v) \rightarrow\left(x^{\prime}, v^{\prime}\right)$ but also $\left(x^{\prime},-v^{\prime}\right) \rightarrow(x,-v)$
\end{itemize}
Compared to Metropolis-Hastings, Hamiltonian Monte Carlo generate the momentum and move to a new postion, which is the new state $x^\prime$.
\subsubsection*{Leap-frog integrator}
Like we just introduced, we want to move to a new state $(x^\prime,v^\prime)$. The numerical approximation is finished by leap-frog integrator. One complete leap-frog integrator works as follows. Let $\epsilon$ as the step size:
\begin{enumerate}
    \item Momentum half update
    $$v\left(t+\frac{\epsilon}{2}\right)=v(t)+\frac{\epsilon}{2} \frac{\mathrm{d} v}{\mathrm{~d} t}(t)=v(t)-\frac{\epsilon}{2}\left. \frac{\partial E}{\partial x}\right|_{(x(t))}$$
    \item Position full update 
    $$x(t+\epsilon)=x(t)+\epsilon \frac{\mathrm{d} x}{\mathrm{~d} t}(t)=x(t)+\epsilon \left.\frac{\partial K}{\partial v}\right|_{\left(v\left(t+\frac{\epsilon}{2}\right)\right)}$$
    \item Momentum full update
    $$v(t+\epsilon)=v\left(t+\frac{\epsilon}{2}\right)-\frac{\epsilon}{2} \left.\frac{\partial E}{\partial x}\right|_{(x(t+\epsilon))}$$
\end{enumerate}
The resulting $x(t+\epsilon)$ and $v(t+\epsilon)$ are the $x^\prime$ and $v^\prime$. We do a fixed number of leap-frog steps.
\subsubsection*{Full HMC algorithm}
The HMC algorithm (run until it mixes):
\begin{itemize}
    \item Current position: $\left.\left(x^{(t-1)}, v^{(t-1)}\right)\right)$
    \item Sample momentum: $v^{(t)} \sim \mathcal{N}(0, I)$.
    \item Start at $(x, v)=\left(x^{(t-1)}, v^{(t)}\right)$ and run Leapfrog integrator for $L$ steps and reach $\left(x^{\prime}, v^{\prime}\right)$
    \item Accept new state $\left(x^{\prime},-v^{\prime}\right)$ with probability:
    $$
    \min \Biggl\{1, \underbrace{\frac{\exp \left(H\left(x^{(t-1)}, v^{(t-1)}\right)\right)}{\exp \left(H\left(x^{\prime}, v^{\prime}\right)\right)}}_{\mathcal{B}}\Biggl\}
    $$
    Similar to Metropolis-hastings. For $\mathcal{B}$:
    \begin{itemize}
        \item If $\mathcal{B}>1$, then accept the new state $\prime{x}$.
        \item If $\mathcal{B}<1$, we sample $u\sim N(0,1)$. Accept if $u\leq\mathcal{B}$; otherwise reject.
    \end{itemize}
    \item \textbf{Low energy} points are favored.
\end{itemize}

\subsection{MCMC Diagnostics}
We use $\hat{R}$ and $\hat{n}_{eff}$ to diagnose MCMC.
\begin{itemize}
    \item Once $\hat{R}$ is near 1 , and $\hat{n}_{\text {eff }}$ is more than 10 per chain for all scalar estimands we collect the mn simulations, (excluding the burn-in).
    \item We can then draw inference based on our samples. However:
    \item Even if the iterative simulations appear to have converged, passed all tests etc. It may still be far from convergence!
    \item When we declare "convergence" - we mean that all chains appear stationary and well mixed.
\end{itemize}